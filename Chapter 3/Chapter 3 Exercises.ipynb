{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1 \n",
    "  \n",
    "## Question:\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions,\n",
    "and rewards. Make the three examples as _different_ from each other as possible. The framework is abstract and\n",
    "flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "## Answer:\n",
    "Example 1: Hairdresser agent\n",
    "\n",
    "- States: the state of the hair and the desired cut of the client. The state of the hair and desired cut could be\n",
    "encoded as arrays of the length of the hair in a set of predetermined areas the head is divided in.\n",
    "- Actions: using the scissors or the clipper (and what accessory) and the area to be cut.\n",
    "- Rewards: negative rewards for the client complaints or imprecisions in the cut and positive rewards for tips.\n",
    "\n",
    "Example 2: DJ agent\n",
    "\n",
    "- States: a measure of how much people are dancing and singing to the song being played and the song currently playing.\n",
    "- Actions: given a setlist of 5000 songs, selecting the next song to be played (or a combination of them) and a type of\n",
    "transition between the songs.\n",
    "- Rewards: negative rewards given by people leaving the club faster than expected, positive rewards given by the level\n",
    "of danciness/_singiness_ in the club.\n",
    "\n",
    "Example 3: Texas Hold'em Poker player agent\n",
    "\n",
    "- States: The two cards in its hand and the cards showing in the table.\n",
    "- Actions: Check, call, raise, or fold.\n",
    "- Rewards: The money obtained or lost after playing one hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 \n",
    "  \n",
    "## Question:\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed learning tasks?\n",
    "Can you think of any clear exceptions?\n",
    "\n",
    "## Answer:\n",
    "We can try to think about it in terms of the limitations a finite MDP imposes in a problem definition and whether any\n",
    "approximations could exist within the framework.\n",
    "\n",
    "1) The Markov property, if not only the previous state and action selected influence which will be the next state. An\n",
    "example of this could be a modified game of chess where the order in which the pieces were moved affects the possible\n",
    "moves. This could be encoded within the MDP, an state would be composed of the position of the pieces and when\n",
    "they have been moved (skyrocketing the amount of available states making it a more difficult learning environment).\n",
    "If the sequence information wasn't available (or only partially available) at the time of making a decision,\n",
    "information from the past that can affect the next state would be missing, breaking the Markov property.\n",
    "\n",
    "2) The action and state sets must be finite. Any problem with infinite available actions or states would need\n",
    "alternative representations, such as grouping them into subsets and use these sets. An example of this could be a\n",
    "problem where the states are the natural numbers and we have to define intervals (e.g. negative numbers, numbers in the\n",
    "range 0-25, 25-200, and 200-âˆž) or where the actions can be a string of any size and we restrict it to a discrete number\n",
    "of lengths (e.g. only generate strings with length 3, 5, 8, 13, 21 and 34).\n",
    "\n",
    "3) Rewards must be numerical. If the rewards the environment gives back are not numerical we would need to encode them\n",
    "as numbers. This can be a highly difficult task as the rewards may not translate naturally to numbers. For example,\n",
    "if the rewards were your family's verbal feedback on how good the meal you prepared was, it would be difficult to\n",
    "convert it into a number and capture correctly its intensity (e.g. What's a better feedback? \"It was really good\" or\n",
    "\"I have enjoyed the meal a lot\"). If we opt for a simpler reward encoding, distinguishing only negative, neutral and\n",
    "positive comments, this may not capture perfectly the information given.\n",
    "\n",
    "The first example is, as far as I understand, a clear exception of the MDP-framework not being an appropriate\n",
    "representation. Apart from this, some of the points mentioned (or their combination) may difficult largely the task for\n",
    "an agent, making it really hard to learn anything valuable from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3\n",
    "  \n",
    "## Question:\n",
    "Consider the problem of driving. You could define the actions in\n",
    "terms of the accelerator, steering wheel, and brake, that is, where your body meets\n",
    "the machine. Or you could define them farther out - say, where the rubber meets the\n",
    "road, considering your actions to be tire torques. Or you could define them farther\n",
    "in - say, where your brain meets your body, the actions being muscle twitches to\n",
    "control your limbs. Or you could go to a really high level and say that your actions\n",
    "are your choices of where to drive. What is the right level, the right place to draw\n",
    "the line between agent and environment? On what basis is one location of the line\n",
    "to be preferred over another? Is there any fundamental reason for preferring one\n",
    "location over another, or is it a free choice?\n",
    "\n",
    "## Answer:\n",
    "The limit of the actions should be at the functional point, being the \n",
    "point at which is the agent makes the decision to take an action that\n",
    "the action always occurs in the same way every time. Above that point\n",
    "it is better to think of the agent as having sub-goals and goals, where\n",
    "something like walking to the door is a goal with the sub-goals of \n",
    "moving the legs, with the action of applying hydraulic pressure to\n",
    "particular points.\n",
    "\n",
    "However this does depend on the reliability of the system, and what\n",
    "reliability you require. If 99% of sub-goals are successful then it may\n",
    "be easy to convert them into actions. Abstracting in this way makes\n",
    "reaching larger goals easier as it dramatically reduces the\n",
    "action-space needed to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4\n",
    "\n",
    "# Question:\n",
    "Give a table analogous to that in Example 3.3, but for p(s',r|s, a). It should have columns for s, a, s', r, and p(s', r|s, a), and a row for every 4-tuple for which p(s',r|s, a) > 0.\n",
    "\n",
    "# Answer\n",
    "Since there is a single reward defined for each triplet (s, a, s'), the table is the same filtering the lines with p(s'|s,a)=0.\n",
    "\n",
    "This fulfills the formula (3.4):\n",
    "\\begin{equation*}\n",
    "p(s'|s, a) = \\sum_{r \\in R} p(s',r|s, a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.5\n",
    "\n",
    "### Question:\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to\n",
    "apply to episodic tasks. Show that you know the modifications needed by giving the modified version\n",
    "of (3.3)\n",
    "\n",
    "### Answer:\n",
    "The original formula is:\n",
    "    \n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S, a \\in A(s)\n",
    "\\end{equation*}\n",
    "\n",
    "according to the definitions in 3.3, for episodic tasks the set of terminal and non-terminal states can be denoted as S+. Therefore, the formula changes to:\n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S^+, a \\in A(s)\n",
    "\\end{equation*}\n",
    "as the dynamics of the MDP in an episodic task include as a possible transition those ending in a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.6\n",
    "  \n",
    "### Question:\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted,\n",
    "continuing formulation of this task?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "The formula would change to:\n",
    "\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "In the limit (very large T), both returns would be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.7\n",
    "  \n",
    "## Question:\n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero at all\n",
    "other times. The task seems to break down naturally into episodes - the successive\n",
    "runs through the maze - so you decide to treat it as an episodic task, where the goal\n",
    "is to maximize expected total reward (3.1). After running the learning agent for a\n",
    "while, you find that it is showing no improvement in escaping from the maze. What\n",
    "is going wrong? Have you effectively communicated to the agent what you want it\n",
    "to achieve?\n",
    "\n",
    "## Answer:\n",
    "This is likely an exploration issue where the agent is unable to find\n",
    "the exit the first time and therefore doesn't know there's anything\n",
    "better than 0 as a reward. Potential solutions include having each\n",
    "non-goal state be worth -1, and/or extending the episode length. This\n",
    "would mean states the agents visits a lot (particularly around the start)\n",
    "will get worse and worse values so it will want to move away from there\n",
    "and eventually find the goal (essentially reaching the goal stops it\n",
    "being in pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.8\n",
    "\n",
    "# Answer\n",
    "Suppose gamma = 0.5 and the following sequence of rewards is received R_1 = -1, R_2 = 2, R_3 = 6, R_4 = 3 and R_5 = 3, with T = 5. What are G_0, G_1, ..., G_5? Hint: Work backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_0 = 2.0\n",
      "G_1 = 6.0\n",
      "G_2 = 8.0\n",
      "G_3 = 4.0\n",
      "G_4 = 2.0\n"
     ]
    }
   ],
   "source": [
    "r = [-1, 2, 6, 3, 2]\n",
    "gamma = 0.5\n",
    "\n",
    "for g_i in range(len(r)):\n",
    "    ret = sum([gamma**i * r_i for i, r_i in enumerate(r[g_i:])])    \n",
    "    print(\"G_\"+ str(g_i) + \" = \" + str(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.9\n",
    "\n",
    "## Question\n",
    "Suppose gamma=0.9 and the reward sequence is R_1 = 2 followed by an infinite sequence of 7s. What are G_1 and G_0?\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "G_0 = 2 + 7 \\frac{1}{1 - \\gamma} = 72\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "G_1 = 7 + 7 \\frac{1}{1 - \\gamma} = 77\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.10\n",
    "\n",
    "## Question\n",
    "Prove (3.10)\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^\\infty y_k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) = lim_{n \\rightarrow \\infty} \\frac{(1 +   \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} = lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} = \\frac{1}{1 - \\gamma}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
